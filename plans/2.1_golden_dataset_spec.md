---
task_id: golden-dataset-llm
type: FEAT
complexity: S
current_phase: READY_TO_IMPLEMENT
completed_phases: [P0, P0.B, P1, P2, P2.A]
branch: feature/golden-dataset-llm
audit_iteration: 1
created_at: 2026-01-09
updated_at: 2026-01-10
---

# 2.1 黄金数据集准备（LLM 辅助）

> Phase 2 的前置任务：构建评估基准，用于度量和改进 MD&A 提取质量。

## P0: 问题定义

### 类型与复杂度

**类型**: FEAT-S (标准新功能)

### 背景 (Why)

当前 MD&A 提取器 (`mda_extractor.py`) 缺乏客观的质量评估体系：
- 现有 `scorer.py` 仅基于关键词命中和省略号计数，无法判断边界准确性
- 无标注数据集，无法计算 Precision / Recall / F1
- 无法量化不同策略的效果差异

**核心价值**: 黄金数据集是所有后续优化工作的基准线。

### 范围 (What)

**做**:
1. 构建 100 份 MD&A 标注样本（stock_code, year, 正确的 start_page, end_page）
2. 实现 LLM-as-Judge 评估器，利用 `claude -p` 命令评估提取质量
3. 编写评估脚本，输出 Precision / Recall / F1 报告

**不做**:
- 不搭建独立 LLM API 客户端（使用 `claude -p` 命令行模式）
- 不实现热加载或在线学习
- 不构建 WebUI 标注界面

### 完成标准 (Done)

| ID | 验收项 | Given | When | Then |
|----|--------|-------|------|------|
| G-01 | 黄金集存在 | 数据目录就绪 | 查看 `data/golden_set.json` | 包含 100 条记录，每条有 stock_code, year, start_char_offset, end_char_offset |
| G-02 | LLM 初标 | 待标注样本列表 | 执行 `scripts/llm_annotate.py` | 输出 LLM 建议的边界，写入 `data/golden_set_draft.json` |
| G-03 | 评估器可用 | 黄金集 + 提取结果 | 执行 `scripts/evaluate_extraction.py` | 输出完整性/准确性评分 + 理由 |
| G-04 | 指标报告 | 黄金集 + 提取结果 | 执行评估脚本 | 输出 Precision / Recall / F1 |
| G-05 | 低分标记 | 评估完成 | 查询 `mda_text.quality_flags` | 低分样本标记 `FLAG_LLM_LOW_SCORE` |

---

## P1: 技术调研

### 1. 现有数据状态

| 数据源 | 数量 | 备注 |
|--------|------|------|
| `mda_text` 表 | 1 条 | 已有提取结果 |
| 已转换 TXT | 1 份 | `outputs/annual_reports/txt/` |
| 年报链接表 | 0 | 需通过爬虫获取 |

**结论**: 现有数据不足以构建 100 份黄金数据集。需要先：
1. 通过 `1.report_link_crawler.py` 爬取年报链接
2. 通过 `2.pdf_batch_converter.py` 下载并转换

### 2. 样本选取策略

**推荐方案**: 分层抽样

| 维度 | 分层 | 数量分配 |
|------|------|----------|
| 年份 | 2018-2023 (6年) | 各 15-17 份 |
| 行业 | 多元化覆盖 | 制造业/金融/科技等 |
| 难度 | 简单/中等/困难 | 各 ~33% |

**"困难样本"识别信号**:
- 目录页不完整或无页码
- 章节标题非标准命名
- 存在多个"经营情况讨论"段落
- 已知提取失败/低分样本

### 3. LLM 评估 Prompt 设计

使用 `claude -p` 命令行模式（无需搭建 API 客户端）。

**评估 Prompt 结构**:
```
你是 MD&A 提取质量评估专家。

## 输入
1. 年报全文（或关键章节）
2. 提取的 MD&A 文本

## 评估维度（各 25 分，满分 100）
1. **完整性**: MD&A 核心内容是否完整？
2. **准确性**: 是否包含非 MD&A 内容（财务报表/其他章节）？
3. **边界准确**: 起止位置是否正确？
4. **噪音控制**: 页眉页脚/表格碎片是否去除？

## 输出格式 (JSON)
{
  "total_score": 85,
  "completeness": {"score": 22, "reason": "..."},
  "accuracy": {"score": 25, "reason": "..."},
  "boundary": {"score": 18, "reason": "..."},
  "noise_control": {"score": 20, "reason": "..."},
  "suggestions": ["..."]
}
```

### 4. 边界匹配容差

| 匹配方式 | 容差 | 适用场景 |
|----------|------|----------|
| 页码匹配 | ±1 页 | 粗粒度评估 |
| 关键词对齐 | 首尾 100 字符包含核心标题 | 细粒度评估 |

**推荐**: 采用"关键词对齐"，因为页码可能因 PDF 解析差异而不准确。

### 5. 技术方案确定

| 组件 | 选型 | 理由 |
|------|------|------|
| LLM 调用 | `claude -p` CLI | 无需搭建 API 客户端，P0 范围限定 |
| 黄金集存储 | `data/golden_set.json` | JSON 格式，便于手工审核 |
| 评估脚本 | `scripts/evaluate_extraction.py` | 独立脚本，不污染核心模块 |
| 标注脚本 | `scripts/llm_annotate.py` | LLM 初标 + 输出 draft |

### 6. 影响范围

| 文件 | 改动类型 | 风险等级 |
|------|----------|----------|
| `scripts/llm_annotate.py` | 新增 | 低 |
| `scripts/evaluate_extraction.py` | 新增 | 低 |
| `data/golden_set.json` | 新增 | 低 |
| `data/golden_set_draft.json` | 新增 | 低 |
| `annual_report_mda/scorer.py` | 可能扩展 | 中 |

### 7. 前置依赖

- [ ] 需先准备 100 份已转换 TXT 样本（通过爬虫+下载+转换）
- [ ] 确认 `claude` CLI 可用

---

## P2: 设计规格

### 1. 数据模型

#### 1.1 黄金数据集 Schema (`data/golden_set.json`)

```json
{
  "version": "1.0",
  "created_at": "2026-01-10T12:00:00",
  "samples": [
    {
      "id": "GS-001",
      "stock_code": "600519",
      "company_name": "贵州茅台",
      "year": 2023,
      "source_txt_path": "outputs/annual_reports/2023/txt/600519_贵州茅台_2023.txt",
      "golden_boundary": {
        "start_marker": "第三节 管理层讨论与分析",
        "end_marker": "第四节 公司治理",
        "start_char_offset": 12345,
        "end_char_offset": 98765,
        "char_count": 86420
      },
      "annotation": {
        "method": "llm_assisted",
        "llm_model": "claude-sonnet",
        "human_verified": true,
        "verified_by": "jeff",
        "verified_at": "2026-01-10T14:00:00"
      },
      "difficulty": "medium",
      "tags": ["制造业", "白酒"]
    }
  ]
}
```

#### 1.2 评估结果 Schema (`data/evaluation_results.json`)

```json
{
  "evaluation_id": "EVAL-20260110-001",
  "evaluated_at": "2026-01-10T15:00:00",
  "extractor_version": "0.3.0",
  "summary": {
    "total_samples": 100,
    "avg_score": 82.5,
    "precision": 0.85,
    "recall": 0.88,
    "f1": 0.865
  },
  "results": [
    {
      "sample_id": "GS-001",
      "extracted_boundary": {
        "start_char_offset": 12300,
        "end_char_offset": 98800
      },
      "llm_evaluation": {
        "total_score": 85,
        "completeness": {"score": 22, "reason": "缺少财务指标解读部分"},
        "accuracy": {"score": 25, "reason": "无非 MD&A 内容混入"},
        "boundary": {"score": 18, "reason": "起始位置偏移 45 字符"},
        "noise_control": {"score": 20, "reason": "存在少量页码噪音"}
      },
      "boundary_match": {
        "start_offset_diff": -45,
        "end_offset_diff": 35,
        "is_acceptable": true
      }
    }
  ]
}
```

### 2. 接口定义

#### 2.1 LLM 标注脚本 (`scripts/llm_annotate.py`)

```python
def annotate_sample(
    txt_path: str,
    stock_code: str,
    year: int,
    model: str = "claude-sonnet"
) -> dict:
    """
    使用 LLM 识别 MD&A 边界。

    Args:
        txt_path: 年报 TXT 文件路径
        stock_code: 股票代码
        year: 年份
        model: LLM 模型标识

    Returns:
        {
            "start_marker": str,
            "end_marker": str,
            "start_char_offset": int,
            "end_char_offset": int,
            "confidence": float,
            "reasoning": str
        }

    Raises:
        FileNotFoundError: TXT 文件不存在
        LLMCallError: LLM 调用失败
    """

def batch_annotate(
    sample_list: list[dict],
    output_path: str = "data/golden_set_draft.json",
    concurrency: int = 1
) -> None:
    """
    批量标注样本列表。

    Args:
        sample_list: [{"txt_path": str, "stock_code": str, "year": int}, ...]
        output_path: 输出 draft JSON 路径
        concurrency: 并发数 (默认 1，避免 API 限流)
    """
```

#### 2.2 评估脚本 (`scripts/evaluate_extraction.py`)

```python
def evaluate_single(
    golden_sample: dict,
    extracted_result: dict,
    use_llm_judge: bool = True
) -> dict:
    """
    评估单个样本的提取质量。

    Args:
        golden_sample: 黄金集中的样本定义
        extracted_result: 提取器输出的结果
        use_llm_judge: 是否使用 LLM 评估（否则仅用规则评估）

    Returns:
        {
            "sample_id": str,
            "llm_evaluation": {...} | None,
            "rule_evaluation": {
                "boundary_match": bool,
                "char_overlap_ratio": float
            },
            "final_score": float
        }
    """

def run_evaluation(
    golden_set_path: str = "data/golden_set.json",
    extraction_source: str = "duckdb",
    output_path: str = "data/evaluation_results.json"
) -> dict:
    """
    运行完整评估流程。

    Args:
        golden_set_path: 黄金数据集路径
        extraction_source: 提取结果来源 ("duckdb" | "json")
        output_path: 评估结果输出路径

    Returns:
        汇总统计 {precision, recall, f1, avg_score}
    """

def call_llm_judge(
    golden_sample: dict,
    extracted_result: dict,
    model: str = "claude-sonnet"
) -> dict:
    """
    调用 LLM 评估提取质量。

    Args:
        golden_sample: 黄金集样本（含 source_txt_path）
        extracted_result: 提取器输出的 MD&A 文本
        model: LLM 模型标识

    Returns:
        {
            "total_score": int,  # 0-100
            "completeness": {"score": int, "reason": str},
            "accuracy": {"score": int, "reason": str},
            "boundary": {"score": int, "reason": str},
            "noise_control": {"score": int, "reason": str},
            "suggestions": list[str]
        }

    Raises:
        LLMCallError: LLM 调用失败
    """
```

### 3. 状态流转

```
[待标注样本] --llm_annotate--> [LLM Draft]
      |                            |
      |                        [人工审核]
      |                            |
      v                            v
[黄金数据集] <--merge_verified-- [已验证样本]
      |
      |--run_extraction-->
      |
[提取结果] --evaluate--> [评估报告]
      |
      v
[低分样本] --flag_quality--> [mda_text.quality_flags]
```

### 4. 关键逻辑伪代码

#### 4.1 LLM 标注核心逻辑

```python
def annotate_sample(txt_path, stock_code, year, model):
    # 1. 读取文件
    if not Path(txt_path).exists():
        raise FileNotFoundError(f"TXT not found: {txt_path}")

    content = Path(txt_path).read_text(encoding="utf-8")

    # 2. 截取关键部分 (避免超出 token 限制)
    # 取前 80000 字符（约覆盖 MD&A 所在区域）
    truncated = content[:80000]

    # 3. 构建 Prompt
    prompt = f"""
    分析以下年报文本，识别"管理层讨论与分析"(MD&A) 章节的边界。

    ## 年报信息
    - 股票代码: {stock_code}
    - 年份: {year}

    ## 年报内容 (前 80000 字符)
    {truncated}

    ## 任务
    找出 MD&A 章节的:
    1. 起始标记（章节标题）
    2. 结束标记（下一章节标题）
    3. 起始字符位置（0-indexed）
    4. 结束字符位置

    ## 输出格式 (JSON)
    {{"start_marker": "...", "end_marker": "...", "start_char_offset": N, "end_char_offset": M, "confidence": 0.0-1.0, "reasoning": "..."}}
    """

    # 4. 调用 claude CLI
    result = subprocess.run(
        ["claude", "-p", prompt, "--output-format", "json"],
        capture_output=True, text=True, timeout=120
    )

    if result.returncode != 0:
        raise LLMCallError(f"claude CLI failed: {result.stderr}")

    # 5. 解析返回
    return json.loads(result.stdout)
```

#### 4.2 评估核心逻辑

```python
def evaluate_single(golden_sample, extracted_result, use_llm_judge):
    sample_id = golden_sample["id"]

    # 1. 规则评估（边界匹配）
    golden_start = golden_sample["golden_boundary"]["start_char_offset"]
    golden_end = golden_sample["golden_boundary"]["end_char_offset"]

    extracted_start = extracted_result.get("start_char_offset", 0)
    extracted_end = extracted_result.get("end_char_offset", 0)

    start_diff = abs(extracted_start - golden_start)
    end_diff = abs(extracted_end - golden_end)

    # 容差: 200 字符
    TOLERANCE = 200
    boundary_match = start_diff <= TOLERANCE and end_diff <= TOLERANCE

    # 计算重叠率
    overlap_start = max(golden_start, extracted_start)
    overlap_end = min(golden_end, extracted_end)
    overlap_len = max(0, overlap_end - overlap_start)
    golden_len = golden_end - golden_start
    overlap_ratio = overlap_len / golden_len if golden_len > 0 else 0

    rule_eval = {
        "boundary_match": boundary_match,
        "char_overlap_ratio": overlap_ratio,
        "start_offset_diff": extracted_start - golden_start,
        "end_offset_diff": extracted_end - golden_end
    }

    # 2. LLM 评估（可选）
    llm_eval = None
    if use_llm_judge:
        llm_eval = call_llm_judge(golden_sample, extracted_result)

    # 3. 综合评分
    if llm_eval:
        final_score = llm_eval["total_score"]
    else:
        # 规则评分: 重叠率 * 100
        final_score = overlap_ratio * 100

    return {
        "sample_id": sample_id,
        "llm_evaluation": llm_eval,
        "rule_evaluation": rule_eval,
        "final_score": final_score
    }
```

### 5. 测试用例设计

| 场景 | 输入 | 预期 | 优先级 |
|------|------|------|--------|
| 正常标注 | 有效 TXT | 返回边界 JSON | P0 |
| 文件不存在 | 无效路径 | FileNotFoundError | P0 |
| LLM 超时 | 大文件 | 重试或 TimeoutError | P1 |
| 边界完全匹配 | golden == extracted | score = 100 | P0 |
| 边界容差内 | diff < 200 | boundary_match = True | P0 |
| 边界超出容差 | diff > 200 | boundary_match = False | P0 |
| 空提取结果 | extracted = {} | overlap_ratio = 0 | P1 |

### 6. CLI 命令设计

```bash
# LLM 初标
python scripts/llm_annotate.py \
  --input samples.json \
  --output data/golden_set_draft.json \
  --model claude-sonnet

# 运行评估
python scripts/evaluate_extraction.py \
  --golden data/golden_set.json \
  --source duckdb \
  --output data/evaluation_results.json \
  --use-llm-judge

# 查看评估摘要
python scripts/evaluate_extraction.py --summary data/evaluation_results.json
```

---

## 变更记录

| 日期 | 变更内容 |
|------|---------|
| 2026-01-09 | 创建计划文档 (P0) |
| 2026-01-10 | 完成 P1 技术调研、P2 设计规格 |
| 2026-01-10 | P2.A 审计通过，修正 G-01 术语、补充 call_llm_judge 接口 |

---

## P2.A 审计报告

**审计时间**: 2026-01-10
**审计结果**: ✅ 通过

### 问题清单

#### 重要级 (已修正)
- [S001] P0 与 P2 术语不一致 → 已统一为 `start_char_offset, end_char_offset`
- [S002] `call_llm_judge` 函数未定义 → 已补充接口定义

#### 优化级 (记录待改进)
- [O001] 缺少 LLM 错误重试策略 → 实现时补充
- [O002] Precision/Recall 计算方式未明确 → 实现时补充

### 历史教训检查
- H001 DuckDB 并发: 不适用
- H002 状态机设计: 已覆盖 ✅
- H003 Pydantic 版本: 不适用
